import os
import time
import random
import requests
import re
import sys
import shutil

# =================== å®‰è£…ä¾èµ– ===================
def install_dependencies():
    """è‡ªåŠ¨å®‰è£…ç¼ºå°‘çš„ä¾èµ–"""
    try:
        import pip
        required_packages = ["selenium", "webdriver-manager", "beautifulsoup4", "requests"]
        for package in required_packages:
            if not package_installed(package):
                print(f"ğŸ“¦ æ­£åœ¨å®‰è£…ä¾èµ–: {package}...")
                pip.main(['install', package])
    except Exception as e:
        print(f"âŒ ä¾èµ–å®‰è£…å¤±è´¥: {e}")
        sys.exit(1)

def package_installed(package_name):
    """æ£€æŸ¥ Python åŒ…æ˜¯å¦å·²å®‰è£…"""
    import importlib.util
    return importlib.util.find_spec(package_name) is not None

install_dependencies()  # è¿è¡Œå®‰è£…

from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.edge.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# ä¾èµ–å®‰è£…å®Œæˆåï¼Œå¯¼å…¥åº“
from webdriver_manager.microsoft import EdgeChromiumDriverManager

# =================== é…ç½®åŒº ===================
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))  # å½“å‰æ–‡ä»¶å¤¹
DOWNLOAD_PATH = os.path.join(CURRENT_DIR, "download_novels")  # å°è¯´å­˜æ”¾ç›®å½•
os.makedirs(DOWNLOAD_PATH, exist_ok=True)

# è®©ç”¨æˆ·è¾“å…¥ COOKIE
COOKIE = input("è¯·ç²˜è´´ä½ çš„ Pixiv COOKIE: ").strip()

# è‡ªåŠ¨æå– USER_ID
match = re.search(r"user_id=(\d+)", COOKIE)
if match:
    USER_ID = match.group(1)
    print(f"ğŸ” ä» COOKIE ä¸­æå–åˆ° USER_ID: {USER_ID}")
else:
    print("âŒ æ— æ³•ä» COOKIE ä¸­è·å– USER_IDï¼Œè¯·æ£€æŸ¥ä½ çš„ COOKIEã€‚")
    sys.exit(1)

# è®©ç”¨æˆ·è¾“å…¥èµ·å§‹é¡µç 
start_page = input("è¯·è¾“å…¥èµ·å§‹é¡µç ï¼ˆå¦‚ 1 æˆ– 51 ç»§ç»­çˆ¬å–ï¼‰: ").strip()
start_page = int(start_page) if start_page.isdigit() else 1

BASE_URL = f"https://www.pixiv.net/users/{USER_ID}/bookmarks/novels?p={{page}}"

# è‡ªåŠ¨æ£€æµ‹ WebDriver
def get_edge_driver_path():
    """å°è¯•è·å– Edge WebDriver è·¯å¾„"""
    possible_paths = [
        shutil.which("msedgedriver"),  # åœ¨ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­æŸ¥æ‰¾
        os.path.join(CURRENT_DIR, "msedgedriver.exe"),  # å½“å‰ç›®å½•
    ]
    for path in possible_paths:
        if path and os.path.exists(path):
            return path
    return None

EDGE_DRIVER_PATH = get_edge_driver_path()

if not EDGE_DRIVER_PATH:
    try:
        # è‡ªåŠ¨ä¸‹è½½å®‰è£… WebDriver
        EDGE_DRIVER_PATH = EdgeChromiumDriverManager().install()
        print(f"âœ… WebDriver å·²å®‰è£…: {EDGE_DRIVER_PATH}")
    except Exception as e:
        print(f"âŒ æ— æ³•å®‰è£… WebDriver: {e}")
        sys.exit(1)
else:
    print(f"âœ… ä½¿ç”¨æœ¬åœ° WebDriver: {EDGE_DRIVER_PATH}")

# è¾…åŠ©å‡½æ•°ï¼šæ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦ä»¥åŠå¤šä½™ç©ºç™½
def safe_filename(name):
    """
    æ›¿æ¢æ‰ Windows æ–‡ä»¶åä¸­ä¸å…è®¸çš„å­—ç¬¦ï¼ˆ\ / : * ? " < > |ï¼‰ä¸ºä¸‹åˆ’çº¿ï¼Œ
    åˆå¹¶å¤šä½™ç©ºæ ¼å’Œä¸‹åˆ’çº¿ï¼Œå¹¶å»é™¤é¦–å°¾ç©ºæ ¼å’Œä¸‹åˆ’çº¿ã€‚
    """
    name = re.sub(r'[\\/*?:"<>|]', '_', name)
    name = re.sub(r'\s+', ' ', name)
    name = re.sub(r'_+', '_', name)
    return name.strip(" _")

# =============================================

class PixivNovelCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.base_url = "https://www.pixiv.net"
        self.headers = {'User-Agent': 'Mozilla/5.0'}
        self.setup_session()

    def setup_session(self):
        """è®¾ç½® requests ä¼šè¯çš„ Cookie"""
        for cookie_pair in COOKIE.split(';'):
            cookie_pair = cookie_pair.strip()
            if not cookie_pair or '=' not in cookie_pair:
                continue
            name, value = cookie_pair.split('=', 1)
            self.session.cookies.set(name, value)

    def get_all_favorites_ids(self, start_page=1):
        """
        ä»æŒ‡å®šé¡µç å¼€å§‹çˆ¬å–æ”¶è—å¤¹ä¸­çš„å°è¯´ IDï¼Œå¹¶ä¸‹è½½å°è¯´ã€‚
        å¦‚æœè¿”å›ç©ºé›†åˆï¼Œåˆ™è®¤ä¸ºå·²åˆ°è¾¾æœ€åä¸€é¡µï¼Œç»ˆæ­¢ç¿»é¡µä¸‹è½½ã€‚
        """
        novel_ids = set()
        page = start_page

        while True:
            url = BASE_URL.format(page=page)
            print(f"\nğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ: {url}")

            try:
                new_ids = self.get_favorites_ids_from_page(url, requested_page=page)
                if not new_ids:
                    print(f"âœ… ç¬¬ {page} é¡µæ²¡æœ‰å‘ç°æ–°çš„å°è¯´æˆ–å·²åˆ°è¾¾æœ€åä¸€é¡µï¼Œçˆ¬å–ç»“æŸï¼")
                    break
            except Exception as e:
                print(f"âš ï¸ çˆ¬å–ç¬¬ {page} é¡µæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                break

            # å¦‚æœæœ¬é¡µæ‰€æœ‰å°è¯´å‡å·²ä¸‹è½½ï¼Œåˆ™è®¤ä¸ºåˆ°è¾¾æœ€åä¸€é¡µ
            diff_ids = new_ids - novel_ids
            if not diff_ids:
                print(f"âœ… ç¬¬ {page} é¡µæ‰€æœ‰å°è¯´å‡å·²ä¸‹è½½ï¼Œçˆ¬å–ç»“æŸï¼")
                break

            novel_ids.update(diff_ids)
            print(f"ğŸ” å½“å‰å·²å‘ç° {len(novel_ids)} æœ¬å°è¯´")

            for novel_id in diff_ids:
                self.crawl_novel(novel_id)
                sleep_time = random.uniform(1, 5)
                print(f"â³ ç­‰å¾… {sleep_time:.2f} ç§’...")
                time.sleep(sleep_time)

            sleep_time = random.uniform(2, 5)
            print(f"â³ ç­‰å¾… {sleep_time:.2f} ç§’...")
            time.sleep(sleep_time)
            page += 1

        return novel_ids

    def get_favorites_ids_from_page(self, url, requested_page):
        """è§£æå•é¡µæ”¶è—å¤¹ï¼Œè·å–å°è¯´ IDï¼Œå¹¶åˆ¤æ–­æ˜¯å¦è¾¾åˆ°æœ€åä¸€é¡µ"""
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920x1080")
        options.add_argument("--ignore-certificate-errors")
        options.add_argument("--disable-extensions")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--disable-features=msAccountSSO,msWelcomePage")

        service = Service(EDGE_DRIVER_PATH)
        driver = webdriver.Edge(service=service, options=options)
        try:
            driver.get("https://www.pixiv.net")
            time.sleep(3)
            for cookie_pair in COOKIE.split(';'):
                cookie_pair = cookie_pair.strip()
                if not cookie_pair or '=' not in cookie_pair:
                    continue
                name, value = cookie_pair.split('=', 1)
                driver.add_cookie({"name": name, "value": value, "domain": ".pixiv.net"})

            driver.get(url)
            # åˆ¤æ–­æ˜¯å¦è‡ªåŠ¨è·³è½¬å›æœ€åä¸€é¡µ
            current_url = driver.current_url
            match_current = re.search(r'p=(\d+)', current_url)
            if match_current:
                actual_page = int(match_current.group(1))
                if actual_page < requested_page:
                    print(f"å·²åˆ°è¾¾æœ€åä¸€é¡µï¼šè¯·æ±‚é¡µ {requested_page}ï¼Œå½“å‰é¡µ {actual_page}")
                    driver.quit()
                    return set()
            WebDriverWait(driver, 15).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a[href*='/novel/show.php?id=']"))
            )
            for _ in range(random.randint(5, 10)):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(random.uniform(1, 3))
            page_source = driver.page_source
        except Exception as e:
            print(f"âŒ é¡µé¢åŠ è½½æˆ–æ»šåŠ¨è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            driver.quit()
            return set()
        driver.quit()
        soup = BeautifulSoup(page_source, 'html.parser')

        ids = set()
        for a in soup.find_all('a', href=True):
            if "/novel/show.php?id=" in a['href']:
                match = re.search(r'id=(\d+)', a['href'])
                if match:
                    ids.add(match.group(1))
        page_param = url.split('=')[-1]
        print(f"ğŸ“– ç¬¬ {page_param} é¡µæ‰¾åˆ° {len(ids)} æœ¬å°è¯´")
        return ids

    def crawl_novel(self, novel_id):
        """ä¸‹è½½å•ç¯‡å°è¯´"""
        novel_url_ajax = f"{self.base_url}/ajax/novel/{novel_id}"
        response = self.session.get(novel_url_ajax, headers=self.headers)
        if response.status_code != 200:
            print(f"âš ï¸ è·å–å°è¯´ {novel_id} å¤±è´¥")
            return

        novel_data = response.json().get('body', {})
        if not novel_data:
            print(f"âš ï¸ å°è¯´ {novel_id} æ•°æ®è§£æå¤±è´¥")
            return

        title = novel_data.get('title', 'æ— æ ‡é¢˜')
        # æ„é€ å°è¯´å±•ç¤ºé¡µé¢ URL
        novel_url_show = f"{self.base_url}/novel/show.php?id={novel_id}"
        
        # æå–å…ƒæ•°æ®
        metadata = {
            "æ ‡é¢˜": novel_data.get('title', 'æ— æ ‡é¢˜').strip(),
            "ä½œè€…": novel_data.get('userName', 'æœªçŸ¥ä½œè€…').strip(),
            "ä¸Šä¼ æ—¶é—´": novel_data.get('uploadDate', 'æœªçŸ¥æ—¥æœŸ'),
            "æ ‡ç­¾": [tag.get('tag', '') for tag in novel_data.get('tags', {}).get('tags', [])],
            "æè¿°": novel_data.get('description', 'æ— æè¿°').replace("<br />", "\n").strip(),
            "å°è¯´ç½‘å€": novel_url_show
        }
        content = novel_data.get('content', '')

        # ç”Ÿæˆå®‰å…¨æ–‡ä»¶åï¼ˆå¯¹ä½œè€…å’Œæ ‡é¢˜éƒ½è¿›è¡Œæ¸…ç†ï¼‰
        safe_author = safe_filename(metadata["ä½œè€…"])
        safe_title = safe_filename(metadata["æ ‡é¢˜"])
        file_name = os.path.join(DOWNLOAD_PATH, f"[{safe_author}]{safe_title}.txt")
        
        metadata_str = self._format_metadata(metadata)
        full_content = f"{metadata_str}{content}"
        with open(file_name, 'w', encoding='utf-8') as f:
            f.write(full_content)

        print(f"âœ… å°è¯´ {title} ä¸‹è½½å®Œæˆï¼")

    def _format_metadata(self, metadata):
        """æ ¼å¼åŒ–å…ƒæ•°æ®ä¸ºæ–‡æœ¬å—ï¼Œå°†å°è¯´ç½‘å€æ’å…¥åœ¨ä¸Šä¼ æ—¶é—´ä¹‹å"""
        metadata_lines = [
            f"æ ‡é¢˜: {metadata['æ ‡é¢˜']}",
            f"ä½œè€…: {metadata['ä½œè€…']}",
            f"æ ‡ç­¾: {', '.join(metadata['æ ‡ç­¾'])}",
            f"ä¸Šä¼ æ—¶é—´: {metadata['ä¸Šä¼ æ—¶é—´']}",
            f"å°è¯´ç½‘å€: {metadata['å°è¯´ç½‘å€']}",
            f"ç®€ä»‹: {metadata['æè¿°']}",
            "\n----------\n\n"
        ]
        return '\n'.join(metadata_lines)

def main():
    crawler = PixivNovelCrawler()
    crawler.get_all_favorites_ids(start_page=start_page)

if __name__ == "__main__":
    main()
