import os
import subprocess
import time
import random
import re
import sys
import json
import shutil
import tempfile
import importlib.util

# =================== é…ç½®åŒº ===================

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))  # å½“å‰æ–‡ä»¶å¤¹
DOWNLOAD_PATH = os.path.join(CURRENT_DIR, "download_novels")  # å°è¯´å­˜æ”¾ç›®å½•
CONFIG_PATH = os.path.join(CURRENT_DIR, "config.json") # é…ç½®æ–‡ä»¶åœ°å€

DRIVER_PATH = '' # WebDriver è·¯å¾„
browser_type = 2   # 1 ä¸º edge , 2 ä¸º chrome 

# ä¸ºåº”å¯¹pixivåçˆ¬æªæ–½ï¼Œæ¯ç« ä¸‹è½½å‰çš„ç­‰å¾…æ—¶é—´åŒºé—´
min_sleep_time , max_sleep_time = 1.5,2.5  


# ä¾èµ–åº“ï¼ˆæ¨¡å—å: PyPIåŒ…åï¼‰
required_modules = {
    'selenium': 'selenium',
    'webdriver_manager': 'webdriver-manager',
    'bs4': 'beautifulsoup4',
    'fake_useragent': 'fake-useragent',
    'requests': 'requests'
}

# =================== å®‰è£…ä¾èµ– ===================
def install_dependencies():
    """è‡ªåŠ¨å®‰è£…ç¼ºå°‘çš„ä¾èµ–"""
    try:
        for module_name, package_name in required_modules.items():
            if not package_installed(module_name):
                print(f"ğŸ“¦ æ­£åœ¨å®‰è£…ä¾èµ–: {package_name}...")
                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package_name])
    except Exception as e:
        print(f"âŒ ä¾èµ–å®‰è£…å¤±è´¥: {e}")
        sys.exit(1)

def package_installed(package_name):
    """æ£€æŸ¥ Python åŒ…æ˜¯å¦å·²å®‰è£…ï¼ˆæŒ‰æ¨¡å—åæ£€æŸ¥ï¼‰"""
    return importlib.util.find_spec(package_name) is not None

install_dependencies()  # è¿è¡Œå®‰è£…

# ä¾èµ–å®‰è£…å®Œæˆåï¼Œå¯¼å…¥åº“
import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from webdriver_manager.microsoft import EdgeChromiumDriverManager
from selenium.webdriver.edge.service import Service
from selenium.webdriver.edge.options import Options

from webdriver_manager.chrome import ChromeDriverManager  
from selenium.webdriver.chrome.service import Service as ChromeService  
from selenium.webdriver.chrome.options import Options as ChromeOptions

def load_config():
    """ä» config.json è¯»å–ä¿å­˜çš„ WebDriver è·¯å¾„å’Œæµè§ˆå™¨ç±»å‹"""
    try:
        if os.path.exists(CONFIG_PATH):
            with open(CONFIG_PATH, "r") as f:
                config = json.load(f)
                saved_path = config.get("driver_path", "")
                saved_browser = config.get("browser_type", browser_type)
                if os.path.exists(saved_path):
                    return saved_path, saved_browser
    except json.JSONDecodeError:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œå°†é‡æ–°ç”Ÿæˆ")
    except Exception as e:
        print(f"âš ï¸ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    return None

def save_config(driver_path, browser_type):
    """å°† WebDriver è·¯å¾„å’Œæµè§ˆå™¨ç±»å‹å†™å…¥ config.json"""
    try:
        config = {
            "driver_path": driver_path,
            "browser_type": browser_type
        }
        with open(CONFIG_PATH, "w") as f:
            json.dump(config, f, indent=2)
        print(f"ğŸ“ é…ç½®å·²ä¿å­˜åˆ°: {CONFIG_PATH}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

def get_driver_path(saved_path):
    """æ£€æµ‹æœ¬åœ°å¯èƒ½çš„ WebDriver è·¯å¾„"""
    possible_paths = [
        saved_path,  # ä¼˜å…ˆçº§1ï¼šé…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„
        shutil.which("chromedriver") if browser_type == 2 else shutil.which("msedgedriver"),  # ä¼˜å…ˆçº§2ï¼šç¯å¢ƒå˜é‡
        os.path.join(CURRENT_DIR, "chromedriver.exe" if browser_type == 2 else "msedgedriver.exe"),  # ä¼˜å…ˆçº§3ï¼šå½“å‰ç›®å½•
    ]
    for path in possible_paths:
        if path and os.path.exists(path):
            return path
    return None

def check_DRIVER_PATH(DRIVER_PATH):
    try:
        if browser_type == 2 and not DRIVER_PATH:
            DRIVER_PATH = ChromeDriverManager().install()
        elif browser_type == 1 and not DRIVER_PATH:
            DRIVER_PATH = EdgeChromiumDriverManager().install()
        print(f"âœ… WebDriver å·²å®‰è£…: {DRIVER_PATH}")
        return DRIVER_PATH
    except Exception as e:
            print(f"âŒ æ— æ³•å®‰è£… WebDriver: {e}")
            sys.exit(1)
    
# è¾…åŠ©å‡½æ•°ï¼šæ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦ä»¥åŠå¤šä½™ç©ºç™½
def safe_filename(name):
    """
    æ›¿æ¢æ‰ Windows æ–‡ä»¶åä¸­ä¸å…è®¸çš„å­—ç¬¦ï¼ˆ\ / : * ? " < > |ï¼‰ä¸ºä¸‹åˆ’çº¿ï¼Œ
    åˆå¹¶å¤šä½™ç©ºæ ¼å’Œä¸‹åˆ’çº¿ï¼Œå¹¶å»é™¤é¦–å°¾ç©ºæ ¼å’Œä¸‹åˆ’çº¿ã€‚
    """
    name = re.sub(r'[\\/*?:"<>|]', '_', name)
    name = re.sub(r'\s+', ' ', name)
    name = re.sub(r'_+', '_', name)
    return name.strip(" _")

# =============================================

class PixivNovelCrawler:
    def __init__(self,COOKIE):
        self.cookie=COOKIE
        self.session = requests.Session()
        self.mode=1 # 1 ä¸º å•ç«  , 2 ä¸º ç³»åˆ—
        self.browser_type = 1 # 1 ä¸º edge , 2 ä¸º chrome 
        self.base_url = "https://www.pixiv.net"
        self.ua = UserAgent(fallback='Mozilla/5.0')
        self.headers = {'User-Agent': self.ua.random}
        self.base_headers={'User-Agent': 'Mozilla/5.0'}
        self.setup_session()
        # è®°å½•å·²ä¸‹è½½çš„ç³»åˆ—ï¼Œé¿å…é‡å¤ä¸‹è½½
        self.downloaded_series = set()

    def setmode(self,mode):
        self.mode=mode
    
    def set_browser_type(self,browser_type):
        self.browser_type=browser_type
    
    def setup_session(self):
        """è®¾ç½® requests ä¼šè¯çš„ Cookie"""
        for cookie_pair in self.cookie.split(';'):
            cookie_pair = cookie_pair.strip()
            if not cookie_pair or '=' not in cookie_pair:
                continue
            name, value = cookie_pair.split('=', 1)
            self.session.cookies.set(name, value)

    def get_all_favorites_ids(self, BASE_URL,start_page=1):
        """
        ä»æŒ‡å®šé¡µç å¼€å§‹çˆ¬å–æ”¶è—å¤¹ä¸­çš„å°è¯´ IDï¼Œå¹¶ä¸‹è½½å°è¯´ã€‚
        å¦‚æœè¿”å›ç©ºé›†åˆï¼Œåˆ™è®¤ä¸ºå·²åˆ°è¾¾æœ€åä¸€é¡µï¼Œç»ˆæ­¢ç¿»é¡µä¸‹è½½ã€‚
        """
        novel_ids = set()
        page = start_page

        while True:
            url = BASE_URL.format(page=page)
            print(f"\nğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ: {url}")

            try:
                new_ids = self.get_favorites_ids_from_page(url, requested_page=page)
                if not new_ids:
                    print(f"âœ… ç¬¬ {page} é¡µæ²¡æœ‰å‘ç°æ–°çš„å°è¯´æˆ–å·²åˆ°è¾¾æœ€åä¸€é¡µï¼Œçˆ¬å–ç»“æŸï¼")
                    break
            except Exception as e:
                print(f"âš ï¸ çˆ¬å–ç¬¬ {page} é¡µæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                break

            # å¦‚æœæœ¬é¡µæ‰€æœ‰å°è¯´å‡å·²ä¸‹è½½ï¼Œåˆ™è®¤ä¸ºåˆ°è¾¾æœ€åä¸€é¡µ
            diff_ids = new_ids - novel_ids
            if not diff_ids:
                print(f"âœ… ç¬¬ {page} é¡µæ‰€æœ‰å°è¯´å‡å·²ä¸‹è½½ï¼Œçˆ¬å–ç»“æŸï¼")
                break
            
            novel_ids.update(diff_ids)
            print(f"ğŸ” å½“å‰å·²å‘ç° {len(novel_ids)} æœ¬å°è¯´")

            for novel_id in diff_ids:
                self.crawl_novel(novel_id)
                sleep_time = random.uniform(min_sleep_time, max_sleep_time)
                print(f"â³ ç­‰å¾… {sleep_time:.2f} ç§’...")
                time.sleep(sleep_time)

            sleep_time = random.uniform(min_sleep_time, max_sleep_time)
            print(f"â³ ç­‰å¾… {sleep_time:.2f} ç§’...")
            time.sleep(sleep_time)
            page += 1

        return novel_ids
    def _generate_random_headers(self):
        """ç”ŸæˆåŒ…å«éšæœº User-Agent çš„ headers"""
        try:
            # ç”Ÿæˆéšæœº User-Agent
            user_agent = self.ua.random
        except Exception as e:
            print(f"ç”Ÿæˆ User-Agent å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤å€¼")
            user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        
        # åˆå¹¶åŸºç¡€ headers å’Œéšæœº User-Agent
        headers = {**self.base_headers, "User-Agent": user_agent}
        return headers
    

    def get_favorites_ids_from_page(self, url, requested_page):
        """è§£æå•é¡µæ”¶è—å¤¹ï¼Œè·å–å°è¯´ IDï¼Œå¹¶åˆ¤æ–­æ˜¯å¦è¾¾åˆ°æœ€åä¸€é¡µ"""
        if self.browser_type == 2:
            options = ChromeOptions()
            driver_path = DRIVER_PATH
            service = ChromeService(driver_path)
        else:
            options = Options()
            driver_path = DRIVER_PATH
            service = Service(driver_path)
        
        options.add_argument("--headless")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920x1080")
        options.add_argument("--log-level=3")  # 0: é»˜è®¤, 1: è­¦å‘Š, 2: ä¿¡æ¯, 3: é”™è¯¯


        # ç”Ÿæˆå”¯ä¸€çš„ä¸´æ—¶ç”¨æˆ·ç›®å½•ï¼Œé¿å…å¤šä¸ª WebDriver å ç”¨
        temp_user_data_dir = tempfile.mkdtemp()
        options.add_argument(f"--user-data-dir={temp_user_data_dir}")

        service = Service(DRIVER_PATH)

        try:
            driver = webdriver.Chrome(service=service, options=options) if self.browser_type == "chrome" \
                else webdriver.Edge(service=service, options=options)
            driver.get("https://www.pixiv.net")
            time.sleep(2)
            
            for cookie_pair in self.cookie.split(';'):
                cookie_pair = cookie_pair.strip()
                if not cookie_pair or '=' not in cookie_pair:
                    continue
                name, value = cookie_pair.split('=', 1)
                driver.add_cookie({"name": name, "value": value, "domain": ".pixiv.net"})

            driver.get(url)
            WebDriverWait(driver, 15).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a[href*='/novel/show.php?id=']"))
            )

            for _ in range(random.randint(4, 5)):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(random.uniform(1.5, 3))

            page_source = driver.page_source

        except Exception as e:
            print(f"âŒ é¡µé¢åŠ è½½æˆ–æ»šåŠ¨è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return set()
        
        finally:
            driver.quit()  # å…³é—­ WebDriverï¼Œé¿å…è¿›ç¨‹æ®‹ç•™
            shutil.rmtree(temp_user_data_dir, ignore_errors=True)  # åˆ é™¤ä¸´æ—¶ç”¨æˆ·ç›®å½•

        soup = BeautifulSoup(page_source, 'html.parser')

        ids = set()
        for a in soup.find_all('a', href=True):
            if "/novel/show.php?id=" in a['href']:
                match = re.search(r'id=(\d+)', a['href'])
                if match:
                    ids.add(match.group(1))

        page_param = url.split('=')[-1]
        print(f"ğŸ“– ç¬¬ {page_param} é¡µæ‰¾åˆ° {len(ids)} æœ¬å°è¯´")
        return ids

    def crawl_novel(self, novel_id):
        """ä¸‹è½½å•ç¯‡å°è¯´"""
        self.headers = self._generate_random_headers()
        novel_url = f"{self.base_url}/ajax/novel/{novel_id}"
        response = self.session.get(novel_url, headers=self.headers)
        if response.status_code != 200:
            print(f"âš ï¸ è·å–å°è¯´ {novel_id} å¤±è´¥")
            return

        novel_data = response.json().get('body', {})
        if not novel_data:
            print(f"âš ï¸ å°è¯´ {novel_id} æ•°æ®è§£æå¤±è´¥")
            return
        
        # æ£€æŸ¥æ–‡ç« æ˜¯å¦å±äºæŸä¸ªç³»åˆ—
        if self.mode==2:
            series_nav_data = novel_data.get('seriesNavData') or {}
            series_id = series_nav_data.get('seriesId')
        else: 
            series_id=''
        
        if series_id:
            self.crawl_series(series_id)
            return  # ä¸ç»§ç»­å¤„ç†å•ç¯‡
        
        # æå–å…ƒæ•°æ®
        metadata = {
            "æ ‡é¢˜": novel_data.get('title', 'æ— æ ‡é¢˜').strip(),
            "ä½œè€…": novel_data.get('userName', 'æœªçŸ¥ä½œè€…').strip(),
            "ä¸Šä¼ æ—¶é—´": novel_data.get('uploadDate', 'æœªçŸ¥æ—¥æœŸ'),
            "æ ‡ç­¾": [tag.get('tag', '') for tag in novel_data.get('tags', {}).get('tags', [])],
            "æè¿°": novel_data.get('description', 'æ— æè¿°').replace("<br />", "\n").strip(),
            "åŸæ–‡ç½‘å€": f"{self.base_url}/novel/show.php?id={novel_id}"
        }
        content = novel_data.get('content', '')

        # ç”Ÿæˆå®‰å…¨æ–‡ä»¶åï¼ˆå¯¹ä½œè€…å’Œæ ‡é¢˜éƒ½è¿›è¡Œæ¸…ç†ï¼‰
        safe_author = safe_filename(metadata["ä½œè€…"])
        safe_title = safe_filename(metadata["æ ‡é¢˜"])
        file_name = os.path.join(DOWNLOAD_PATH, f"[{safe_author}]{safe_title}.txt")
        
        metadata_str = self._format_metadata(metadata)
        full_content = f"{metadata_str}{content}"
        with open(file_name, 'w', encoding='utf-8') as f:
            f.write(full_content)

        print(f"âœ… å°è¯´ {safe_title} ä¸‹è½½å®Œæˆï¼")



    def _format_metadata(self, metadata):
        """æ ¼å¼åŒ–å…ƒæ•°æ®ä¸ºæ–‡æœ¬å—"""
        metadata_lines = [
            f"æ ‡é¢˜: {metadata['æ ‡é¢˜']}",
            f"ä½œè€…: {metadata['ä½œè€…']}",
            f"æ ‡ç­¾: {', '.join(metadata['æ ‡ç­¾'])}",
            f"ä¸Šä¼ æ—¶é—´: {metadata['ä¸Šä¼ æ—¶é—´']}",
            f"åŸæ–‡ç½‘å€: {metadata['åŸæ–‡ç½‘å€']}",
            f"ç®€ä»‹:\n{metadata['æè¿°']}".rstrip(),
            "\n" + "="*20 + "\n\n"
        ]
        return '\n'.join(metadata_lines)

    def crawl_series(self, series_id):
        """ç³»åˆ—å¤„ç†æ–¹æ³•"""
        # å¦‚æœç³»åˆ—å·²ä¸‹è½½ï¼Œåˆ™è·³è¿‡
        if series_id in self.downloaded_series:
            print(f"âœ… ç³»åˆ— {series_id} å·²ä¸‹è½½ï¼Œè·³è¿‡...")
            return  # é¿å…é‡å¤ä¸‹è½½åŒä¸€ç³»åˆ—
        
        self.downloaded_series.add(series_id)  # æ ‡è®°å·²ä¸‹è½½
        
        # è·å–ç³»åˆ—å…ƒæ•°æ®
        time.sleep(random.uniform(min_sleep_time, max_sleep_time))  # é™ä½è¯·æ±‚é¢‘ç‡
        series_info = self._get_series_info(series_id)
        if not series_info or not series_info['chapters']:
            return

        # æ”¶é›†ç« èŠ‚å†…å®¹
        chap_num=1
        for chap in series_info['chapters']:
            print(f"â³ å·²ä¸‹è½½ç³»åˆ— {chap_num}/{series_info['total']} ç« ", end="\r")
            chap['content'] = self._get_chapter_text(chap['id'])
            chap_num+=1
            time.sleep(random.uniform(min_sleep_time, max_sleep_time))
            
        # åˆå¹¶ä¿å­˜
        self._save_combined_series(series_info)

    def _get_series_info(self, series_id):
        self.headers = self._generate_random_headers()
        """è·å–ç³»åˆ—åŸºç¡€å…ƒæ•°æ®"""
        info_url = f"{self.base_url}/ajax/novel/series/{series_id}"
        try:
            response = self.session.get(info_url, headers=self.headers)
            if response.status_code != 200:
                return None
            data = response.json().get('body', {})
            
            series_info = {
                'title': data.get('title', 'æ— æ ‡é¢˜').strip(),
                'author': data.get('userName', 'æœªçŸ¥ä½œè€…').strip(),
                'url':f"{self.base_url}/novel/series/{series_id}",
                'desc': data.get('caption', 'æ— ç®€ä»‹').replace("<br />", "\n").strip(),
                'total': data.get('total', 0),
                'tags': data.get('tags', []),
                'chapters': []
            }
        except Exception as e:
            print(f"âš ï¸ è·å–ç³»åˆ—ä¿¡æ¯å¤±è´¥: {e}")
            return None
        
        print(f"ğŸ“š æ£€æµ‹åˆ°ç³»åˆ— {series_info['title']} ï¼Œå¼€å§‹å¤„ç†")
        # åˆ†é¡µè·å–ç« èŠ‚å…ƒæ•°æ®
        self.headers = self._generate_random_headers()
        limit = 30  # Pixiv æ¯é¡µå›ºå®šè¿”å›30æ¡
        last_order = 0
        while last_order < series_info['total']:
            content_url = f"{self.base_url}/ajax/novel/series_content/{series_id}"
            params = {
                'limit': limit,
                'last_order': last_order,
                'order_by': 'asc'
            }
            try:
                content_res = self.session.get(content_url, params=params, headers=self.headers)
                content_data = content_res.json().get('body', {}).get('page', {}).get('seriesContents', [])
                
                # è§£æç« èŠ‚æ•°æ®
                for item in content_data:
                    series_info['chapters'].append({
                        'id': item['id'],
                        'title': item['title'],
                        'comment': item['commentHtml'].replace("<br />", "\n"),
                        'order': int(item['series']['contentOrder']),
                        'content':''
                    })
                
                last_order += len(content_data)
                time.sleep(random.uniform(min_sleep_time, max_sleep_time))  # é™ä½è¯·æ±‚é¢‘ç‡
                
                # Pixiv å®é™…å¯èƒ½è¿”å›å°‘äºlimitçš„æƒ…å†µ
                if len(content_data) < limit:
                    break
                    
            except Exception as e:
                print(f"âš ï¸ è·å–ç« èŠ‚åˆ—è¡¨å¤±è´¥: {e}")
                break    
        # æŒ‰orderæ’åºç« èŠ‚
        series_info['chapters'].sort(key=lambda x: x['order'])
        return series_info            

    def _get_chapter_text(self, novel_id):
        """è·å–ç« èŠ‚æ­£æ–‡"""
        self.headers = self._generate_random_headers()
        url = f"{self.base_url}/ajax/novel/{novel_id}"
        try:
            response = self.session.get(url, headers=self.headers)
            return response.json().get('body', {}).get('content', '')
        except:
            return "ã€ç« èŠ‚å†…å®¹åŠ è½½å¤±è´¥ã€‘"

    def _save_combined_series(self, series_info):
        """åˆå¹¶ä¿å­˜æ–‡ä»¶"""
        # ç”Ÿæˆæ–‡ä»¶å
        safe_author = safe_filename(series_info['author'])
        safe_title = safe_filename(series_info['title'])
        filename = f"[{safe_author}]{safe_title}.txt"
        filepath = os.path.join(DOWNLOAD_PATH, filename)
        
        if series_info['total']>1:
            series_title=f"æ ‡é¢˜ï¼š{series_info['title']} (1~{series_info['total']})"

        else:
            series_title=f"æ ‡é¢˜ï¼š{series_info['title']}"

        # æ„å»ºæ–‡ä»¶å¤´
        header = [
            series_title,
            f"ä½œè€…ï¼š{series_info['author']}",
            f"æ ‡ç­¾ï¼š{', '.join(series_info['tags'])}",
            f"åŸæ–‡ç½‘å€: {series_info['url']}",
            f"ç®€ä»‹ï¼š\n{series_info['desc']}".rstrip(),
        ]

        # æŒ‰ç« èŠ‚é¡ºåºå†™å…¥
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(header))
            for chap in series_info['chapters']:
                if chap['comment']!='':
                    comment=f"ä½œè€…çš„è¯ï¼š\n\n" + "-"*10 + f"\n\n{chap['comment']}".rstrip() + f'\n\n'+"-"*10+f'\n'
                else:
                    comment=''
                chapter_content = [
                    "\n\n" + "="*20 + "\n",
                    f"{chap['title']}".rstrip()+f'\n',
                    comment,
                    chap['content'].rstrip()+f'\n'
                ]
                f.write('\n'.join(chapter_content))

        print(f"âœ… å·²ä¿å­˜ç³»åˆ— {series_info['title']} ï¼Œå…± {series_info['total']} ç« ")


def main():
    global DOWNLOAD_PATH,CONFIG_PATH,DRIVER_PATH,browser_type
    os.makedirs(DOWNLOAD_PATH, exist_ok=True)
    
    # è·å– web_driver 
    if os.path.exists(CONFIG_PATH):
        saved_path,browser_type=load_config()
    else:
        print(f"\næ£€æµ‹åˆ°æ­¤æ¬¡ä¸ºåˆæ¬¡è¿è¡Œï¼Œè¯·é€‰æ‹©æ‹¥æœ‰çš„æµè§ˆå™¨ : \nç±»å‹ 1: edge\nç±»å‹ 2: chrome")
        browser_type=input("è¯·è¾“å…¥æµè§ˆå™¨ç±»å‹ï¼ˆé»˜è®¤ä¸º 1 ï¼‰ ").strip()
        browser_type = int(browser_type) if browser_type.isdigit() else 1
        saved_path=''
    
    DRIVER_PATH = get_driver_path(saved_path)
    DRIVER_PATH = check_DRIVER_PATH(DRIVER_PATH)
    save_config(DRIVER_PATH, browser_type)
        
    # é€‰æ‹©æ¨¡å¼
    print(f"\nè¯·é€‰æ‹©ä¸‹è½½æ¨¡å¼ : \næ¨¡å¼ 1: æŒ‰å•ç« ä¸‹è½½\næ¨¡å¼ 2: æŒ‰ç³»åˆ—ä¸‹è½½")
    mode=input("è¯·è¾“å…¥é€‰æ‹©çš„ä¸‹è½½æ¨¡å¼ï¼ˆé»˜è®¤ä¸º 1 ï¼‰ ").strip()
    mode = int(mode) if mode.isdigit() else 1

    
    # è®©ç”¨æˆ·è¾“å…¥ COOKIE
    COOKIE = input("è¯·ç²˜è´´ä½ çš„ Pixiv COOKIE: ").strip()

    # è‡ªåŠ¨æå– USER_ID
    match = re.search(r"user_id=(\d+)", COOKIE)
    if match:
        USER_ID = match.group(1)
        print(f"ğŸ” ä» COOKIE ä¸­æå–åˆ° USER_ID: {USER_ID}")
    else:
        print(f"âŒ æ— æ³•ä» COOKIE ä¸­è·å– USER_IDï¼Œè¯·æ£€æŸ¥ä½ çš„ COOKIEã€‚")
        sys.exit(1)
    

    # è®©ç”¨æˆ·è¾“å…¥èµ·å§‹é¡µç 
    start_page = input("è¯·è¾“å…¥çˆ¬å–çš„èµ·å§‹é¡µç ï¼ˆé»˜è®¤ä¸º 1 ï¼‰: ").strip()
    start_page = int(start_page) if start_page.isdigit() else 1

    BASE_URL = f"https://www.pixiv.net/users/{USER_ID}/bookmarks/novels?p={{page}}"
    crawler = PixivNovelCrawler(COOKIE)
    crawler.setmode(mode)
    crawler.set_browser_type(browser_type)
    crawler.get_all_favorites_ids(BASE_URL,start_page)

if __name__ == "__main__":
    main()

